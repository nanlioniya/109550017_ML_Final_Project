{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install feature_engine","metadata":{"execution":{"iopub.status.busy":"2023-01-09T07:33:03.681897Z","iopub.execute_input":"2023-01-09T07:33:03.682323Z","iopub.status.idle":"2023-01-09T07:33:14.726912Z","shell.execute_reply.started":"2023-01-09T07:33:03.682289Z","shell.execute_reply":"2023-01-09T07:33:14.725895Z"},"trusted":true},"execution_count":388,"outputs":[{"name":"stdout","text":"Requirement already satisfied: feature_engine in /opt/conda/lib/python3.7/site-packages (1.4.0)\nRequirement already satisfied: scikit-learn>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (1.0.2)\nRequirement already satisfied: statsmodels>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (0.13.2)\nRequirement already satisfied: pandas>=1.0.3 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (1.3.5)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (1.21.6)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (1.7.3)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.3->feature_engine) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.3->feature_engine) (2022.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=1.0.0->feature_engine) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=1.0.0->feature_engine) (1.0.1)\nRequirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.7/site-packages (from statsmodels>=0.11.1->feature_engine) (21.3)\nRequirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.7/site-packages (from statsmodels>=0.11.1->feature_engine) (0.5.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=21.3->statsmodels>=0.11.1->feature_engine) (3.0.9)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from patsy>=0.5.2->statsmodels>=0.11.1->feature_engine) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport warnings\nimport feature_engine as fe\nfrom feature_engine.encoding import WoEEncoder\nfrom colorama import Fore, Back, Style\nimport xgboost\nimport random\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.linear_model import LogisticRegression,HuberRegressor\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler,PowerTransformer\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn.metrics import r2_score,roc_auc_score\nfrom imblearn.under_sampling import NearMiss, ClusterCentroids\nfrom imblearn.over_sampling import SMOTE\n\nsns.set()\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-01-09T07:33:14.730015Z","iopub.execute_input":"2023-01-09T07:33:14.730526Z","iopub.status.idle":"2023-01-09T07:33:14.743517Z","shell.execute_reply.started":"2023-01-09T07:33:14.730472Z","shell.execute_reply":"2023-01-09T07:33:14.741774Z"},"trusted":true},"execution_count":389,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/tabular-playground-series-aug-2022/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-aug-2022/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-01-09T07:33:14.744777Z","iopub.execute_input":"2023-01-09T07:33:14.745651Z","iopub.status.idle":"2023-01-09T07:33:14.916448Z","shell.execute_reply.started":"2023-01-09T07:33:14.745605Z","shell.execute_reply":"2023-01-09T07:33:14.915194Z"},"trusted":true},"execution_count":390,"outputs":[]},{"cell_type":"code","source":"target = train.pop('failure')\ndata = pd.concat([train, test])\nfeature = [f for f in data.columns if f.startswith('measurement') or f=='loading']\nnullValue_cols = [col for col in train.columns if train[col].isnull().sum()!=0]","metadata":{"execution":{"iopub.status.busy":"2023-01-09T07:33:14.918974Z","iopub.execute_input":"2023-01-09T07:33:14.919304Z","iopub.status.idle":"2023-01-09T07:33:14.946357Z","shell.execute_reply.started":"2023-01-09T07:33:14.919274Z","shell.execute_reply":"2023-01-09T07:33:14.945133Z"},"trusted":true},"execution_count":391,"outputs":[]},{"cell_type":"code","source":"def data_pre_processing(data):\n\n    data['m3_missing'] = data['measurement_3'].isnull().astype(np.int8)\n    data['m5_missing'] = data['measurement_5'].isnull().astype(np.int8)\n    data['area'] = data['attribute_2'] * data['attribute_3']\n    data['loading'] = np.log(data['loading']) #The loading feature seems to have right skewed distribution.\n                                              #Let's apply log transformation to make the distribution more normal.\n    data['count_null'] = data.isnull().sum(axis=1)\n\n    full_fill_dict ={}\n    full_fill_dict['measurement_17'] = {\n        'A': ['measurement_5','measurement_6','measurement_8','measurement_7'],\n        'B': ['measurement_4','measurement_5','measurement_7','measurement_9'],\n        'C': ['measurement_5','measurement_7','measurement_8','measurement_9'],\n        'D': ['measurement_5','measurement_6','measurement_7','measurement_8'],\n        'E': ['measurement_4','measurement_5','measurement_6','measurement_8'],\n        'F': ['measurement_4','measurement_5','measurement_6','measurement_7'],\n        'G': ['measurement_4','measurement_6','measurement_8','measurement_9'],\n        'H': ['measurement_4','measurement_5','measurement_7','measurement_8','measurement_9'],\n        'I': ['measurement_3','measurement_7','measurement_8','measurement_9']\n    }\n\n\n    # collect the name of the next 10 best measurement columns sorted by correlation (except 17 already done above):\n    col = [col for col in test.columns if 'measurement' not in col]+ ['loading','m3_missing','m5_missing']\n    a = []\n    b =[]\n\n    for x in range(3,17):\n        corr = np.absolute(data.drop(col, axis=1).corr()[f'measurement_{x}']).sort_values(ascending=False)\n        a.append(np.round(np.sum(corr[1:4]),3)) # we add the 3 first lines of the correlation values to get the \"most correlated\"\n        b.append(f'measurement_{x}')\n\n    c = pd.DataFrame()\n    c['Selected columns'] = b\n    c['correlation total'] = a\n    c = c.sort_values(by = 'correlation total',ascending=False).reset_index(drop = True)\n    # calculating top 4 correlated features for each measurement column w.r.t each product code\n\n    for i in range(10):\n        measurement_col = 'measurement_' + c.iloc[i,0][12:] # we select the next best correlated column \n        fill_dict = {}\n        for x in data.product_code.unique() : \n            corr = np.absolute(data[data.product_code == x].drop(col, axis=1)\n                               .corr()[measurement_col]).sort_values(ascending=False)\n            measurement_col_dic = {}\n            measurement_col_dic[measurement_col] = corr[1:5].index.tolist()\n            fill_dict[x] = measurement_col_dic[measurement_col]\n        full_fill_dict[measurement_col] =fill_dict\n\n    for code in data.product_code.unique():\n        total_na_filled_by_linear_model = 0\n        for measurement_col in list(full_fill_dict.keys()):\n            tmp = data[data.product_code == code]\n            column = full_fill_dict[measurement_col][code]\n            tmp_train = tmp[column+[measurement_col]].dropna(how='any')\n            tmp_test = tmp[(tmp[column].isnull().sum(axis=1)==0)&(tmp[measurement_col].isnull())]\n\n            model = HuberRegressor(epsilon=1.5)\n            model.fit(tmp_train[column], tmp_train[measurement_col])\n            data.loc[(data.product_code==code)&(data[column].isnull().sum(axis=1)==0)\n                     &(data[measurement_col].isnull()),measurement_col] = model.predict(tmp_test[column])\n            total_na_filled_by_linear_model += len(tmp_test)\n\n        # others NA columns:\n        NA = data.loc[data[\"product_code\"] == code,nullValue_cols ].isnull().sum().sum()\n        model1 = KNNImputer(n_neighbors=3)\n        data.loc[data.product_code==code, feature] = model1.fit_transform(data.loc[data.product_code==code, feature])\n\n    data['measurement_avg'] = data[[f'measurement_{i}' for i in range(3, 17)]].mean(axis=1)\n    df_train = data.iloc[:train.shape[0],:]\n    df_test = data.iloc[train.shape[0]:,:]\n\n    woe_encoder = WoEEncoder(variables=['attribute_0'])\n    woe_encoder.fit(df_train, target)\n    df_train = woe_encoder.transform(df_train)\n    df_test = woe_encoder.transform(df_test)\n    df_train['measurement(3*5)'] = df_train['measurement_3'] * df_train['measurement_5']\n    df_test['measurement(3*5)'] = df_test['measurement_3'] * df_test['measurement_5']\n    df_train['missing(3*5)'] = df_train['m5_missing'] * (df_train['m3_missing'])\n    df_test['missing(3*5)'] = df_test['m5_missing'] * (df_test['m3_missing'])\n    \n    return df_train, df_test","metadata":{"execution":{"iopub.status.busy":"2023-01-09T07:33:14.947867Z","iopub.execute_input":"2023-01-09T07:33:14.948206Z","iopub.status.idle":"2023-01-09T07:33:14.972497Z","shell.execute_reply.started":"2023-01-09T07:33:14.948175Z","shell.execute_reply":"2023-01-09T07:33:14.971437Z"},"trusted":true},"execution_count":392,"outputs":[]},{"cell_type":"code","source":"df_train, df_test = data_pre_processing(data)\ndf_train['failure'] = target","metadata":{"execution":{"iopub.status.busy":"2023-01-09T07:33:14.973915Z","iopub.execute_input":"2023-01-09T07:33:14.974239Z","iopub.status.idle":"2023-01-09T07:33:38.267823Z","shell.execute_reply.started":"2023-01-09T07:33:14.974210Z","shell.execute_reply":"2023-01-09T07:33:38.266606Z"},"trusted":true},"execution_count":393,"outputs":[]},{"cell_type":"code","source":"def scale(train_data, val_data, test_data, feats):\n    scaler = StandardScaler()\n    scaled_train = scaler.fit_transform(train_data[feats])\n    scaled_val = scaler.transform(val_data[feats])\n    scaled_test = scaler.transform(test_data[feats])\n    new_train = train_data.copy()\n    new_val = val_data.copy()\n    new_test = test_data.copy()\n    new_train[feats] = scaled_train\n    new_val[feats] = scaled_val\n    new_test[feats] = scaled_test\n    return new_train, new_val, new_test","metadata":{"execution":{"iopub.status.busy":"2023-01-09T07:33:38.269387Z","iopub.execute_input":"2023-01-09T07:33:38.271022Z","iopub.status.idle":"2023-01-09T07:33:38.277626Z","shell.execute_reply.started":"2023-01-09T07:33:38.270981Z","shell.execute_reply":"2023-01-09T07:33:38.276670Z"},"trusted":true},"execution_count":394,"outputs":[]},{"cell_type":"code","source":"pickled_model = pickle.load(open('Model.sav', 'rb'))","metadata":{"execution":{"iopub.status.busy":"2023-01-09T07:33:38.279096Z","iopub.execute_input":"2023-01-09T07:33:38.279516Z","iopub.status.idle":"2023-01-09T07:33:38.291643Z","shell.execute_reply.started":"2023-01-09T07:33:38.279481Z","shell.execute_reply":"2023-01-09T07:33:38.290401Z"},"trusted":true},"execution_count":395,"outputs":[]},{"cell_type":"code","source":"# features = ['loading','attribute_0' , 'measurement_17', 'measurement_0', 'measurement_1','measurement_2','area', 'm3_missing', 'm5_missing', \n#         'measurement_avg','measurement(3*5)','missing(3*5)'] # ,'count_null','ohe_a_7', 'ohe_a_6', 'ohe_a_8','measurement_4','measurement_9','measurement_7','measurement_6','measurement_8'\n\nfeatures = ['loading', 'measurement_17', 'm3_missing', 'm5_missing']\n\nN_FOLDS = 15\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=0)\ny_test = np.zeros(df_test[features].shape[0])\n\nfor train_ind, val_ind in skf.split(df_train[features], df_train[['failure']]):\n    preds_test = pickled_model.predict_proba(test_x)[:,1]\n    y_test = y_test + preds_test / N_FOLDS","metadata":{"execution":{"iopub.status.busy":"2023-01-09T07:33:38.293498Z","iopub.execute_input":"2023-01-09T07:33:38.294015Z","iopub.status.idle":"2023-01-09T07:33:38.393879Z","shell.execute_reply.started":"2023-01-09T07:33:38.293967Z","shell.execute_reply":"2023-01-09T07:33:38.392270Z"},"trusted":true},"execution_count":396,"outputs":[]},{"cell_type":"code","source":"sub_log = pd.read_csv('../input/tabular-playground-series-aug-2022/sample_submission.csv')\nsub_log['failure'] = y_test\n\nsub_log.to_csv(\"submission.csv\", index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-09T07:33:38.399501Z","iopub.execute_input":"2023-01-09T07:33:38.400898Z","iopub.status.idle":"2023-01-09T07:33:38.526434Z","shell.execute_reply.started":"2023-01-09T07:33:38.400714Z","shell.execute_reply":"2023-01-09T07:33:38.525053Z"},"trusted":true},"execution_count":397,"outputs":[]}]}